{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Un cas d'usage concret de régression avec MAPIE\n",
    "\n",
    "## Contexte\n",
    "\n",
    "La Californie est divisée en districts, pour chacun desquels une licence est requise pour y opérer en tant que professionnel de l'immobilier.* Tous les 10 ans, un recensement de la population est effectué, et les données, publiques, sont une mine d'or pour les promoteurs immobilier de l'état californien.\n",
    "\n",
    "Ces données contiennent les informations suivantes :\n",
    "\n",
    "| Variable   | Description |\n",
    "|------------|------------|\n",
    "| `MedHouseVal` | Valeur médiane des maisons d'un district donné (en centaines de milliers de dollars). |\n",
    "| `MedInc`     | Revenu médian des ménages dans le district (en dizaines de milliers de dollars). |\n",
    "| `HouseAge`   | Âge médian des maisons dans le district (en années). |\n",
    "| `AveRooms`   | Nombre moyen de pièces par logement. |\n",
    "| `AveBedrms`  | Nombre moyen de chambres par logement. |\n",
    "| `Population` | Population totale du district. |\n",
    "| `AveOccup`   | Nombre moyen de personnes par logement. |\n",
    "| `Latitude`   | Latitude du centre du district. |\n",
    "| `Longitude`  | Longitude du centre du district. |\n",
    "\n",
    "Bill Smith, un promoteur sollicite votre aide. En effet, comme lors des recensements précédents, certains districts dits \"sneaky\"* ne souhaitent pas publier les données relatives à la valeur des maisons sur leur périmètre. Cette information est pourtant clé pour estimer s'il est intéressant d'effectuer les démarches pour acquérir une license dans un district donné.\n",
    "\n",
    "Les jeux de données à votre disposition sont donc :\n",
    "  - `X` et `y` : les données des districts ayant publié l'intégralité de leurs chiffres (`y` contient l'information `MedHouseVal`, et `X` les autres variables)\n",
    "  - `X_sneaky` : les données des sneaky districts (sans l'information `MedHouseVal`)\n",
    "\n",
    "Lors du précédent recensement, Mr. Smith a dû mobiliser ses équipes pendant plus d'une semaine pour estimer cette donnée manquante, district par district. Il souhaite cette fois-ci automatiser le processus.\n",
    "\n",
    "Après plusieurs ateliers avec lui, vous avez défini ensemble un livrable sous forme de carte, affichant en vert, les sneaky districts dont la valeur médiane des maisons ne dépasse pas 150 000\\$. Bill semble plutôt averse au risque, et préfère donc obtenir un nombre réduit de district dont on est sûrs.\n",
    "\n",
    "*Note : ce cas d'usage est fictif, nous avons donc pris des libertés avec les lois réellement en vigueur en Californie !\n",
    "\n",
    "## Plan d'action\n",
    "1. Analyse exploratoire rapide des données\n",
    "2. Entrainement d'un modèle GradientBoostingRegressor, car il est puissant et supporte la loss quantile (nous verrons l'intérêt de cette loss plus loin)\n",
    "3. Conformalisation du modèle en utilisant la méthode ConformalizedQuantileRegressor\n",
    "4. Prédire les intervalles de confiance sur le jeu de données X_sneaky pour la variable `MedHouseVal`\n",
    "5. Affichage sur une carte des sneaky districts qui correspondent aux critères de Bill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Imports et chargement du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from scipy.stats import uniform, randint\n",
    "from mapie.metrics import regression_coverage_score, regression_mean_width_score\n",
    "from mapie_v1.regression import ConformalizedQuantileRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"files_cas_d_usage/X.csv\")\n",
    "y = pd.read_csv(\"files_cas_d_usage/y.csv\").squeeze(\"columns\")\n",
    "X_sneaky = pd.read_csv(\"files_cas_d_usage/X_sneaky.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Analyse exploratoire rapide des données (fournie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Valeurs nulles et doublons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Vérifions que le jeu de données X comprend des valeurs nulles ou des doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.info(), \"\\n\")\n",
    "print(\"Doublons :\", X.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Distribution des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=y)\n",
    ")\n",
    "fig.update_layout(\n",
    "    height=350,\n",
    "    width=600,\n",
    "    xaxis_title=\"Prix médian (100 000$)\",\n",
    "    title_text=\"Cible\",\n",
    "    font=dict(family=\"Computer Modern\", size=18, color=\"#7f7f7f\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 3, 3\n",
    "fig = make_subplots(rows=rows, cols=cols, subplot_titles=X.columns)\n",
    "\n",
    "for i, col in enumerate(X.columns):\n",
    "    row = i // cols + 1\n",
    "    col_num = i % cols + 1\n",
    "    fig.add_trace(go.Histogram(y=X[col], name=col), row=row, col=col_num)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=700,\n",
    "    width=1200,\n",
    "    title_text=\"Histograms\",\n",
    "    font=dict(family=\"Computer Modern\", size=18, color=\"#7f7f7f\")\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Corrélations entre les variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = X.corr()\n",
    "\n",
    "fig = ff.create_annotated_heatmap(\n",
    "    z=np.abs(corr_matrix.values),\n",
    "    x=list(corr_matrix.columns),\n",
    "    y=list(corr_matrix.index),\n",
    "    annotation_text=corr_matrix.round(2).values,\n",
    "    showscale=True\n",
    ")\n",
    "fig.update_layout(\n",
    "    font=dict(family=\"Computer Modern\", size=18, color=\"#7f7f7f\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Entrainement d'un modèle prédictif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Séparation des données d'entrainement et de conformalisation\n",
    "\n",
    "Pour les méthodes dites \"split\" incluses dans MAPIE (`SplitConformalRegressor` et `ConformalizedQuantileRegressor`), les jeux d'entrainements et de conformalisation sont utilisés de la façon suivante :\n",
    "\n",
    "![title](files_cas_d_usage/data-split.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "**Exercice 1** : Lors du tutoriel précédent, nous avons utilisé le `ConformalizedQuantileRegressor` en laissant au soin de MAPIE l'entrainement du modèle. Cette fois-ci, nous allons donc utiliser le mode `prefit=True` avec un `GradientBoostingRegressor` entrainé par nos soins. On a donc besoin d'un jeu d'entrainement, d'un jeu de test, et, pour utiliser MAPIE, d'un jeu de conformalisation.\n",
    "  - Créez un jeu d'entrainement (`X_train`, `y_train`) à partir de (`X`, `y`), en gardant 20% des données pour la partie test + conformalisation\n",
    "  - À partir de ces données restantes, créez un jeu de conformalisation (`X_conformalize`, `y_conformalize`) et de test (`X_test`, `y_test`), de tailles égales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "On va utiliser un `GradientBoostingRegressor`, plutôt robuste aux variables corrélées et aux distributions asymétriques\n",
    "\n",
    "**Exercice 2** :\n",
    "  - Faites une recherche d'hyperparamètres sur un modèle sklearn de type `GradientBoostingRegressor`, avec la méthode `RandomizedSearchCV` (qui permet une validation croisée, ou \"cross validation\").\n",
    "  - Ne dépassez pas 50 entrainements (ce nombre dépend de la grille de paramètres recherchés, et du nombre de plis (\"folds\").\n",
    "  - Utilisez `loss=\"absolute_error\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    \"n_estimators\": randint(50, 500),\n",
    "    \"learning_rate\": uniform(0.01, 0.3),\n",
    "    \"max_depth\": randint(2, 10),\n",
    "    \"subsample\": uniform(0.5, 0.5),\n",
    "    \"min_samples_split\": randint(2, 20),\n",
    "    \"min_samples_leaf\": randint(1, 20),\n",
    "    \"max_features\": uniform(0.5, 0.5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "**Exercice 3** : pour le modèle le plus performant :\n",
    "- Affichez ses hyperparamètres\n",
    "- Sa MAE moyenne sur les jeux de validation croisés\n",
    " - Sa MAE sur le jeu de test\n",
    "- L'interpretation de ces valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=y_test, \n",
    "    y=y_pred, \n",
    "    mode=\"markers\", \n",
    "    name=\"Prédictions\",\n",
    "    marker=dict(color=\"blue\", opacity=0.6)\n",
    "))\n",
    "\n",
    "min_val, max_val = np.min(y_test), np.max(y_test)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[min_val, max_val], \n",
    "    y=[min_val, max_val], \n",
    "    mode=\"lines\", \n",
    "    name=\"Ligne y = x\", \n",
    "    line=dict(color=\"red\", dash=\"dash\")\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Comparaison entre y_test et y_pred\",\n",
    "    xaxis_title=\"y_test (100 000$)\",\n",
    "    yaxis_title=\"y_pred (100 000$)\",\n",
    "    showlegend=True,\n",
    "    font=dict(family=\"Computer Modern\", size=18, color=\"#7f7f7f\")\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Calcul des intervalles avec MAPIE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Explication du fonctionnement du `ConformalizedQuantileRegressor`\n",
    "Cette méthode a besoin non pas d'1 mais de 3 modèles : un modèle usuel entrainé à prédire la variable cible (notre modèle entrainé plus haut), et deux modèles entrainés à prédire respectivement les quantiles 5% et 95% de la variable cible. L'idée est que la variable cible se situe 90% du temps entre les prédictions de nos deux modèles quantiles à 5% et 95%.\n",
    "\n",
    "Enfin, la phase de conformalisation permet de \"calibrer\" ces modèles pour fournir les garanties théoriques propres aux prédictions conformes.\n",
    "\n",
    "![title](files_cas_d_usage/quantiles.png)\n",
    "\n",
    "Nous devons donc avant d'utiliser MAPIE entrainer deux modèles supplémentaires correspondants aux quantiles haut et bas.\n",
    "\n",
    "**Exercice 4** : entrainez les deux modèles quantile, avec les mêmes hyper paramètres que le modèle principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_level = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "**Exercice 5** : Par curiosité, intéressons-nous aux taux de couverture empirique sur le jeu de test que ces modèles fournissent, *avant* conformalisation avec MAPIE. Créez `y_pred_lower_quantile_before_mapie` et  `y_pred_upper_quantile_before_mapie`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Taux de couverture empirique avant MAPIE: {regression_coverage_score(y_test, y_pred_lower_quantile_before_mapie, y_pred_upper_quantile_before_mapie):.3f}\")\n",
    "print(f\"Largeur moyenne des intervalles avant MAPIE : {round(regression_mean_width_score(y_pred_lower_quantile_before_mapie, y_pred_upper_quantile_before_mapie), 2)} (100k$)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame((y_pred_upper_quantile_before_mapie - y_pred_lower_quantile_before_mapie), columns=[\"Largeur d'intervalles\"])\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=df[\"Largeur d'intervalles\"]\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title_text=\"Distribution des largeurs d'intervalles (100 000$)\",\n",
    "    xaxis_title=\"Taille d'intervalle (100 000$)\",\n",
    "    font=dict(family=\"Computer Modern\", size=18, color=\"#7f7f7f\")\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Que penser de ce taux de couverture empirique ? Utilisons maintenant MAPIE pour obtenir des garanties de couverture plus robustes :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "**Exercice 6** : utilisez le `ConformalizedQuantileRegressor` en mode `prefit` pour prédire sur `X_test` les prédictions `y_preds_test` et les intervalles `y_pred_intervals_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Vérifions notre taux de couverture empirique sur le jeu de test, et calculons la taille moyenne de nos intervalles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lower_quantile = y_pred_intervals_test[:, 0, 0]\n",
    "y_pred_upper_quantile = y_pred_intervals_test[:, 1, 0]\n",
    "\n",
    "print(f\"Taux de couverture empirique : {regression_coverage_score(y_test, y_pred_lower_quantile, y_pred_upper_quantile):.3f}\")\n",
    "print(f\"Largeur moyenne des intervalles : {round(regression_mean_width_score(y_pred_lower_quantile, y_pred_upper_quantile), 2)} (100 000$)\")\n",
    "\n",
    "intervals_before = (y_pred_upper_quantile_before_mapie - y_pred_lower_quantile_before_mapie)\n",
    "intervals_after = (y_pred_upper_quantile - y_pred_lower_quantile)\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.15, \n",
    "                    subplot_titles=[\"Avant MAPIE\", \"Après MAPIE\"])\n",
    "fig.add_trace(go.Histogram(x=intervals_before, name=\"Avant MAPIE\", nbinsx=20), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=intervals_after, name=\"Après MAPIE\", nbinsx=20), row=2, col=1)\n",
    "fig.update_layout(\n",
    "    title_text=\"Distribution des largeurs d'intervalles (100 000$)\",\n",
    "    showlegend=False,\n",
    "    font=dict(family=\"Computer Modern\", size=18, color=\"#7f7f7f\")\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Le taux de couverture est-il satisfaisant ? Se fait-il au détriment d'un autre indicateur d'intérêt de notre problème ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=y_test[0::30], \n",
    "    y=y_pred[0::30],\n",
    "    error_y=dict(\n",
    "        type=\"data\",\n",
    "        array=intervals_after,\n",
    "    ),\n",
    "    mode=\"markers\", \n",
    "    name=\"Prédictions\",\n",
    "    marker=dict(color=\"blue\", opacity=0.6)\n",
    "))\n",
    "\n",
    "min_val, max_val = np.min(y_test), np.max(y_test)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[min_val, max_val], \n",
    "    y=[min_val, max_val], \n",
    "    mode=\"lines\", \n",
    "    name=\"Ligne y = x\", \n",
    "    line=dict(color=\"red\", dash=\"dash\")\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Comparaison entre y_test et y_pred avec barre d'erreurs (sur un sous-ensemble pour plus de lisibilité)\",\n",
    "    xaxis_title=\"y_test (100 000$)\",\n",
    "    yaxis_title=\"y_pred (100 000$)\",\n",
    "    showlegend=True,\n",
    "    font=dict(family=\"Computer Modern\", size=18, color=\"#7f7f7f\")\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "# Production du livrable sous forme de carte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "**Exercice 7** :\n",
    "- Utilisez MAPIE pour calculer `y_preds_sneaky` et `y_pred_intervals_sneaky`\n",
    "- Créez un array Numpy `green_sneaky_districts` contenant la liste des sneaky districts que nous voulons afficher (`True`) ou non (`False`) à partir de `y_pred_intervals_sneaky`\n",
    "- Créez un Dataframe `map_data` contenant les colonnes `Latitude`, `Longitude`, et `Green district`\n",
    "- Utilisez le code folium pour afficher la carte demandée par Bill\n",
    "\n",
    "![title](files_cas_d_usage/green-districts.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_map = folium.Map(location=[36.7783, -119.4179], zoom_start=6, min_zoom=5)\n",
    "\n",
    "for _, row in map_data.iterrows():\n",
    "    if row[\"Green district\"]:\n",
    "        folium.CircleMarker(\n",
    "            location=[row['Latitude'], row['Longitude']],\n",
    "            radius=3,\n",
    "            color=\"green\",\n",
    "            fill=True,\n",
    "            fill_opacity=0.7,\n",
    "        ).add_to(california_map)\n",
    "\n",
    "n_sneaky_district_total = X_sneaky.shape[0]\n",
    "n_green_district = map_data[\"Green district\"].sum()\n",
    "print(\"Nombre de sneaky districts total\", n_sneaky_district_total)\n",
    "print(\"Nombre de green districts\", n_green_district)\n",
    "print(\"Nombre de districts non affichés sur la carte\", n_sneaky_district_total - n_green_district)\n",
    "\n",
    "california_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "**Exercice 8** : nous avons choisi un taux de couverture de 90%. Quelle garantie statistique pouvons-nous communiquer à Bill à propos des green districts ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "# Retours de Bill\n",
    "\n",
    "Bill est satisfait de ce premier livrable, et comprend bien la signification statistique de vos résultats. Il souhaite continuer à travailler avec vous pour affiner le travail fourni.\n",
    "\n",
    "En particulier, il aimerait connaître les sneaky districts incertains mais prometteurs. C'est-à-dire ceux dont :\n",
    " - la valeur des maisons prédite par le modèle est inférieure à 150 000\\$\n",
    " - mais dont la borne supérieure prédite par MAPIE dépasse ce plafond\n",
    "\n",
    "Vous avez acculturé Bill aux limites de l'IA, et il comprend qu'il devra prendre une décision au cas par cas avec ses équipes pour ces districts.\n",
    "\n",
    "Affinons le livrable en affichant en orange ces districts incertains.\n",
    "\n",
    "![title](files_cas_d_usage/orange-districts.png)\n",
    "\n",
    "**Exercice 9** :\n",
    "- Créez un array Numpy `orange_sneaky_districts` contenant la liste des districts incertains que nous voulons afficher (`True`) ou non (`False`) à partir de `y_preds_sneaky` et  `y_pred_intervals_sneaky`.\n",
    "- Ajouter au Dataframe `map_data` la colonne `Orange district`\n",
    "- Affichez la nouvelle carte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_map = folium.Map(location=[36.7783, -119.4179], zoom_start=6, min_zoom=5)\n",
    "\n",
    "for _, row in map_data.iterrows():\n",
    "    if row[\"Green district\"]:\n",
    "        folium.CircleMarker(\n",
    "            location=[row['Latitude'], row['Longitude']],\n",
    "            radius=3,\n",
    "            color=\"green\",\n",
    "            fill=True,\n",
    "            fill_opacity=0.7,\n",
    "        ).add_to(california_map)\n",
    "\n",
    "for _, row in map_data.iterrows():\n",
    "    if row[\"Orange district\"]:\n",
    "        folium.CircleMarker(\n",
    "            location=[row['Latitude'], row['Longitude']],\n",
    "            radius=1,\n",
    "            color=\"orange\",\n",
    "            fill=True,\n",
    "            fill_opacity=0.7,\n",
    "        ).add_to(california_map)\n",
    "\n",
    "n_orange_district = map_data[\"Orange district\"].sum()\n",
    "print(\"Nombre de sneaky districts total\", n_sneaky_district_total)\n",
    "print(\"Nombre de green districts\", n_green_district)\n",
    "print(\"Nombre de orange districts\", n_orange_district)\n",
    "print(\"Nombre de districts non affichés sur la carte\", n_sneaky_district_total - n_green_district - n_orange_district)\n",
    "\n",
    "california_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Bravo ! Vous êtes désormais un expert des prédictions conformes ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "# Pour aller plus loin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "Quelques questions ouvertes pour prolonger la réflexion autour de ce cas d'usage :\n",
    "1. Imaginons que Bill n'est pas adverse au risque mais plutôt tête brulée : Quels choix de niveau de confiance feriez-vous ? Quel choix de entre green et orange districts ?\n",
    "2. Bill nous informe que le nombre total de chambre est un indicateur d'interêt dans ses calculs de rentabilité. On peut s'intéresser au taux de couverture conditionnel en fonction du nombre total de chambres (par exemple en découpant cette variable en catégories : 0-10, 10-20, 30-40, 40-50). Les taux de couverture correspondants à ces différentes catégories sont-ils homogènes ? Si non, quel moyen de remédiation pourrait-on utiliser ?\n",
    "3. Notre problème s'apparente finalement à une classification binaire, voire ternaire : green, orange et red districts. Comment pourrait-on utiliser MAPIE dans ce cadre (spoiler : les outils de contrôle de risque)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
